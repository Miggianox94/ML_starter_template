{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "As it has been pointed out in the discussion (e.g. https://www.kaggle.com/c/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/discussion/131028), most of the high scoring public kernels have a leak. That includes mine, apparently. The main cause of this is that they (intentionally or not) ignore the chronological order of the data. In this case you end up with having many cases where the future data are used by a model to predict the past, causing a leak.\n",
    "\n",
    "One way to avoid such situations is apparently to use only the past data to predict the future, like using only ~2018 to predict 2019. Here I implemented such a validation strategy. The log_loss in the end should be around 0.5 if you are truely successful, as is always the case with the winners in the past competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "%matplotlib inline\n",
    "import copy\n",
    "import datetime\n",
    "from sklearn.utils import shuffle\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history\n",
    "from sklearn import model_selection\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, accuracy_score, roc_auc_score, log_loss, classification_report, confusion_matrix\n",
    "import json\n",
    "import ast\n",
    "import time\n",
    "from sklearn import linear_model\n",
    "\n",
    "# keras\n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, LambdaCallback\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Layer, Dense, Concatenate, Reshape, Dropout, merge, Add, BatchNormalization, GaussianNoise\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras import backend as K\n",
    "from keras.layers import Layer\n",
    "from keras.callbacks import *\n",
    "import tensorflow as tf\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print(\"Libraries imported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Class\n",
    "Note that this part is different from the original kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, log_loss, mean_squared_error, mean_absolute_error\n",
    "from sklearn import linear_model\n",
    "\n",
    "class RunModel(object):\n",
    "    \"\"\"\n",
    "    Model Fitting and Prediction Class:\n",
    "\n",
    "    train_df : train pandas dataframe\n",
    "    test_df : test pandas dataframe\n",
    "    target : target column name (str)\n",
    "    features : list of feature names\n",
    "    categoricals : list of categorical feature names\n",
    "    model : lgb, xgb, catb, linear, or nn\n",
    "    task : options are ... regression, multiclass, or binary\n",
    "    n_splits : K in KFold (default is 3)\n",
    "    cv_method : options are ... KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold, StratifiedGroupKFold\n",
    "    group : group feature name when GroupKFold or StratifiedGroupKFold are used\n",
    "    parameter_tuning : bool, only for LGB\n",
    "    seed : seed (int)\n",
    "    scaler : options are ... None, MinMax, Standard\n",
    "    verbose : bool\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_df, test_df, target, features, categoricals=[],\n",
    "                model=\"linear\", task=\"regression\", n_splits=3, cv_method=\"KFold\", \n",
    "                group=None, parameter_tuning=False, seed=1220, scaler=None, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.target = target\n",
    "        self.features = features\n",
    "        self.categoricals = categoricals\n",
    "        self.model = model\n",
    "        self.task = task\n",
    "        self.n_splits = n_splits\n",
    "        self.cv_method = cv_method\n",
    "        self.group = group\n",
    "        self.parameter_tuning = parameter_tuning\n",
    "        self.seed = seed\n",
    "        self.scaler = scaler\n",
    "        self.verbose = verbose\n",
    "        self.cv = self.get_cv()\n",
    "        self.params = self.get_params()\n",
    "        self.y_pred, self.score, self.model, self.oof, self.y_val, self.fi_df = self.fit()\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        # verbose\n",
    "        verbosity = 1000 if self.verbose else 0\n",
    "\n",
    "        if self.task == \"regression\":\n",
    "            # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html\n",
    "            model = linear_model.Ridge(**{'alpha': 220, 'solver': 'lsqr', 'fit_intercept': self.params['fit_intercept'],\n",
    "                                    'max_iter': self.params['max_iter'], 'random_state': self.params['random_state']})\n",
    "        elif (self.task == \"binary\") | (self.task == \"multiclass\"):\n",
    "            # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "            model = linear_model.LogisticRegression(**{\"C\": 1.0, \"fit_intercept\": self.params['fit_intercept'], \n",
    "                                    \"random_state\": self.params['random_state'], \"solver\": \"lbfgs\", \"max_iter\": self.params['max_iter'], \n",
    "                                    \"multi_class\": 'auto', \"verbose\":0, \"warm_start\":False})\n",
    "        model.fit(train_set['X'], train_set['y'])\n",
    "\n",
    "        # permutation importance to get a feature importance (off in default)\n",
    "        # fi = PermulationImportance(model, train_set['X'], train_set['y'], self.features)\n",
    "        fi = np.zeros(len(self.features)) # no feature importance computed\n",
    "        \n",
    "        return model, fi # fitted model and feature importance\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {\n",
    "            'max_iter': 5000,\n",
    "            'fit_intercept': True,\n",
    "            'random_state': self.seed\n",
    "        }\n",
    "        return params\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = {'X': x_train, 'y': y_train}\n",
    "        val_set = {'X': x_val, 'y': y_val}\n",
    "        return train_set, val_set\n",
    "\n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "\n",
    "    def calc_metric(self, y_true, y_pred): # this may need to be changed based on the metric of interest\n",
    "        if self.task == \"multiclass\":\n",
    "            return log_loss(y_true, y_pred)\n",
    "        elif self.task == \"binary\":\n",
    "            return log_loss(y_true, y_pred)\n",
    "        elif self.task == \"regression\":\n",
    "            return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "    def get_cv(self):\n",
    "        if self.cv_method == \"KFold\":\n",
    "            cv = KFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n",
    "            return cv.split(self.train_df)\n",
    "        elif self.cv_method == \"StratifiedKFold\":\n",
    "            cv = StratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=self.seed)\n",
    "            return cv.split(self.train_df, self.train_df[self.target])\n",
    "        elif self.cv_method == \"TimeSeriesSplit\":\n",
    "            cv = TimeSeriesSplit(max_train_size=None, n_splits=self.n_splits)\n",
    "            return cv.split(self.train_df)\n",
    "\n",
    "    def fit(self):\n",
    "        # initialize\n",
    "        oof_pred = np.zeros((self.train_df.shape[0], ))\n",
    "        y_vals = np.zeros((self.train_df.shape[0], ))\n",
    "        y_pred = np.zeros((self.test_df.shape[0], ))\n",
    "        if self.group is not None:\n",
    "            if self.group in self.features:\n",
    "                self.features.remove(self.group)\n",
    "            if self.group in self.categoricals:\n",
    "                self.categoricals.remove(self.group)\n",
    "        fi = np.zeros((self.n_splits, len(self.features)))\n",
    "\n",
    "        # scaling, if necessary\n",
    "        if self.scaler is not None:\n",
    "            # fill NaN\n",
    "            numerical_features = [f for f in self.features if f not in self.categoricals]\n",
    "            self.train_df[numerical_features] = self.train_df[numerical_features].fillna(self.train_df[numerical_features].median())\n",
    "            self.test_df[numerical_features] = self.test_df[numerical_features].fillna(self.test_df[numerical_features].median())\n",
    "            self.train_df[self.categoricals] = self.train_df[self.categoricals].fillna(self.train_df[self.categoricals].mode().iloc[0])\n",
    "            self.test_df[self.categoricals] = self.test_df[self.categoricals].fillna(self.test_df[self.categoricals].mode().iloc[0])\n",
    "\n",
    "            # scaling\n",
    "            if self.scaler == \"MinMax\":\n",
    "                scaler = MinMaxScaler()\n",
    "            elif self.scaler == \"Standard\":\n",
    "                scaler = StandardScaler()\n",
    "            df = pd.concat([self.train_df[numerical_features], self.test_df[numerical_features]], ignore_index=True)\n",
    "            scaler.fit(df[numerical_features])\n",
    "            x_test = self.test_df.copy()\n",
    "            x_test[numerical_features] = scaler.transform(x_test[numerical_features])\n",
    "            if self.model == \"nn\":\n",
    "                x_test = [np.absolute(x_test[i]) for i in self.categoricals] + [x_test[numerical_features]]\n",
    "            else:\n",
    "                x_test = x_test[self.features]\n",
    "        else:\n",
    "            x_test = self.test_df[self.features]\n",
    "\n",
    "        # fitting with out of fold\n",
    "        for fold, (train_idx, val_idx) in enumerate(self.cv):\n",
    "            # train test split\n",
    "            x_train, x_val = self.train_df.loc[train_idx, self.features], self.train_df.loc[val_idx, self.features]\n",
    "            y_train, y_val = self.train_df.loc[train_idx, self.target], self.train_df.loc[val_idx, self.target]\n",
    "\n",
    "            # fitting & get feature importance\n",
    "            if self.scaler is not None:\n",
    "                x_train[numerical_features] = scaler.transform(x_train[numerical_features])\n",
    "                x_val[numerical_features] = scaler.transform(x_val[numerical_features])\n",
    "                if self.model == \"nn\":\n",
    "                    x_train = [np.absolute(x_train[i]) for i in self.categoricals] + [x_train[numerical_features]]\n",
    "                    x_val = [np.absolute(x_val[i]) for i in self.categoricals] + [x_val[numerical_features]]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model, importance = self.train_model(train_set, val_set)\n",
    "            fi[fold, :] = importance\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            y_vals[val_idx] = y_val\n",
    "            x_test = self.convert_x(x_test)\n",
    "            if (self.model == \"linear\") & (self.task != \"regression\"):\n",
    "                oofs = model.predict_proba(conv_x_val)\n",
    "                ypred = model.predict_proba(x_test) / self.n_splits\n",
    "            else:\n",
    "                oofs = model.predict(conv_x_val)\n",
    "                ypred = model.predict(x_test) / self.n_splits\n",
    "                if (self.model == \"nn\") & (self.task != \"multiclass\"):\n",
    "                    oofs = oofs.ravel()\n",
    "                    ypred = ypred.ravel()\n",
    "            if len(oofs.shape) == 2:\n",
    "                if oofs.shape[1] == 2:\n",
    "                    oof_pred[val_idx] = oofs[:, -1]\n",
    "                    y_pred += ypred[:, -1]\n",
    "                elif oofs.shape[1] > 2:\n",
    "                    oof_pred[val_idx] = np.argmax(oofs, axis=1)\n",
    "                    y_pred += np.argmax(ypred, axis=1)\n",
    "            else:\n",
    "                oof_pred[val_idx] = oofs.reshape(oof_pred[val_idx].shape)\n",
    "                y_pred += ypred.reshape(y_pred.shape)\n",
    "            print('Partial score of fold {} is: {}'.format(fold, self.calc_metric(y_val, oof_pred[val_idx])))\n",
    "\n",
    "        # feature importance data frame\n",
    "        fi_df = pd.DataFrame()\n",
    "        for n in np.arange(self.n_splits):\n",
    "            tmp = pd.DataFrame()\n",
    "            tmp[\"features\"] = self.features\n",
    "            tmp[\"importance\"] = fi[n, :]\n",
    "            tmp[\"fold\"] = n\n",
    "            fi_df = pd.concat([fi_df, tmp], ignore_index=True)\n",
    "        gfi = fi_df[[\"features\", \"importance\"]].groupby([\"features\"]).mean().reset_index()\n",
    "        fi_df = fi_df.merge(gfi, on=\"features\", how=\"left\", suffixes=('', '_mean'))\n",
    "\n",
    "        # outputs\n",
    "        loss_score = self.calc_metric(y_vals, oof_pred)\n",
    "        if self.verbose:\n",
    "            print('Our oof loss score is: ', loss_score)\n",
    "        return y_pred, loss_score, model, oof_pred, y_vals, fi_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data\n",
    "\n",
    "Let's load all useful data into a single dictionary!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "for i in glob.glob('/kaggle/input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MDataFiles_Stage1/*'):\n",
    "    name = i.split('/')[-1].split('.')[0]\n",
    "    if name != 'MTeamSpellings':\n",
    "        data_dict[name] = pd.read_csv(i)\n",
    "    else:\n",
    "        data_dict[name] = pd.read_csv(i, encoding='cp1252')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have many data..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'Cities'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MTeamCoaches'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MTeamSpellings'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MMasseyOrdinals'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MSeasons'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MTeams'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MSecondaryTourneyTeams'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MNCAATourneyCompactResults'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MGameCities'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'Conferences'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MNCAATourneySeeds'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get int from seed\n",
    "data_dict['MNCAATourneySeeds']['Seed'] = data_dict['MNCAATourneySeeds']['Seed'].apply(lambda x: int(x[1:3]))\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MNCAATourneySlots'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MTeamConferences'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MNCAATourneySeedRoundSlots'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MNCAATourneyDetailedResults'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MConferenceTourneyGames'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MRegularSeasonDetailedResults'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MRegularSeasonCompactResults'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'MSecondaryTourneyCompactResults'\n",
    "print(data_dict[fname].shape)\n",
    "data_dict[fname].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also have a look at test\n",
    "test = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv')\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format ID\n",
    "test = test.drop(['Pred'], axis=1)\n",
    "test['Season'] = test['ID'].apply(lambda x: int(x.split('_')[0]))\n",
    "test['WTeamID'] = test['ID'].apply(lambda x: int(x.split('_')[1]))\n",
    "test['LTeamID'] = test['ID'].apply(lambda x: int(x.split('_')[2]))\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing and feature engineering.\n",
    "\n",
    "The main idea is to extract features, which could be useful to understand how much one team is better than another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge tables ============\n",
    "train = data_dict['MNCAATourneyCompactResults'] # use compact data only for now\n",
    "\n",
    "# # compact <- detailed (Tourney files)\n",
    "# train = pd.merge(data_dict['MNCAATourneyCompactResults'], data_dict['MNCAATourneyDetailedResults'], how='left',\n",
    "#              on=['Season', 'DayNum', 'WTeamID', 'WScore', 'LTeamID', 'LScore', 'WLoc', 'NumOT'])\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train =================================\n",
    "# merge with Game Cities\n",
    "gameCities = pd.merge(data_dict['MGameCities'], data_dict['Cities'], how='left', on=['CityID'])\n",
    "cols_to_use = gameCities.columns.difference(train.columns).tolist() + [\"Season\", \"WTeamID\", \"LTeamID\"]\n",
    "train = train.merge(gameCities[cols_to_use].drop_duplicates(subset=[\"Season\", \"WTeamID\", \"LTeamID\"]),\n",
    "                    how=\"left\", on=[\"Season\", \"WTeamID\", \"LTeamID\"])\n",
    "train.head()\n",
    "\n",
    "# merge with MSeasons\n",
    "cols_to_use = data_dict[\"MSeasons\"].columns.difference(train.columns).tolist() + [\"Season\"]\n",
    "train = train.merge(data_dict[\"MSeasons\"][cols_to_use].drop_duplicates(subset=[\"Season\"]),\n",
    "                    how=\"left\", on=[\"Season\"])\n",
    "train.head()\n",
    "\n",
    "# merge with MTeams\n",
    "cols_to_use = data_dict[\"MTeams\"].columns.difference(train.columns).tolist()\n",
    "train = train.merge(data_dict[\"MTeams\"][cols_to_use].drop_duplicates(subset=[\"TeamID\"]),\n",
    "                    how=\"left\", left_on=[\"WTeamID\"], right_on=[\"TeamID\"])\n",
    "train.drop(['TeamID'], axis=1, inplace=True)\n",
    "train = train.merge(data_dict[\"MTeams\"][cols_to_use].drop_duplicates(subset=[\"TeamID\"]),\n",
    "                    how=\"left\", left_on=[\"LTeamID\"], right_on=[\"TeamID\"], suffixes=('_W', '_L'))\n",
    "train.drop(['TeamID'], axis=1, inplace=True)\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with MTeamCoaches\n",
    "cols_to_use = data_dict[\"MTeamCoaches\"].columns.difference(train.columns).tolist() + [\"Season\"]\n",
    "train = train.merge(data_dict[\"MTeamCoaches\"][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]), \n",
    "                    how=\"left\", left_on=[\"Season\",\"WTeamID\"], right_on=[\"Season\",\"TeamID\"])\n",
    "train.drop(['TeamID'], axis=1, inplace=True)\n",
    "\n",
    "train = train.merge(data_dict[\"MTeamCoaches\"][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]), \n",
    "                    how=\"left\", left_on=[\"Season\",\"LTeamID\"], right_on=[\"Season\",\"TeamID\"], suffixes=('_W', '_L'))\n",
    "train.drop(['TeamID'], axis=1, inplace=True)\n",
    "print(train.shape)\n",
    "train.head()\n",
    "\n",
    "# # merge with MMasseyOrdinals (too heavy for kaggle kernel?)\n",
    "# cols_to_use = data_dict[\"MMasseyOrdinals\"].columns.difference(train.columns).tolist() + [\"Season\"]\n",
    "# train = train.merge(data_dict[\"MMasseyOrdinals\"], how=\"left\", left_on=[\"Season\",\"WTeamID\"], right_on=[\"Season\",\"TeamID\"])\n",
    "# train.drop(['TeamID'], axis=1, inplace=True)\n",
    "# train = train.merge(data_dict[\"MMasseyOrdinals\"], how=\"left\", left_on=[\"Season\",\"LTeamID\"], right_on=[\"Season\",\"TeamID\"], suffixes=('_W', '_L'))\n",
    "# train.drop(['TeamID'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with MNCAATourneySeeds\n",
    "cols_to_use = data_dict['MNCAATourneySeeds'].columns.difference(train.columns).tolist() + ['Season']\n",
    "train = train.merge(data_dict['MNCAATourneySeeds'][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]),\n",
    "                    how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\n",
    "train.drop(['TeamID'], axis=1, inplace=True)\n",
    "train = train.merge(data_dict['MNCAATourneySeeds'][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]),\n",
    "                    how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], suffixes=('_W', '_L'))\n",
    "train.drop(['TeamID'], axis=1, inplace=True)\n",
    "\n",
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test =================================\n",
    "# merge with Game Cities\n",
    "cols_to_use = gameCities.columns.difference(test.columns).tolist() + [\"Season\", \"WTeamID\", \"LTeamID\"]\n",
    "test = test.merge(gameCities[cols_to_use].drop_duplicates(subset=[\"Season\", \"WTeamID\", \"LTeamID\"]),\n",
    "                  how=\"left\", on=[\"Season\", \"WTeamID\", \"LTeamID\"])\n",
    "del gameCities\n",
    "gc.collect()\n",
    "test.head()\n",
    "\n",
    "# merge with MSeasons\n",
    "cols_to_use = data_dict[\"MSeasons\"].columns.difference(test.columns).tolist() + [\"Season\"]\n",
    "test = test.merge(data_dict[\"MSeasons\"][cols_to_use].drop_duplicates(subset=[\"Season\"]),\n",
    "                  how=\"left\", on=[\"Season\"])\n",
    "test.head()\n",
    "\n",
    "# merge with MTeams\n",
    "cols_to_use = data_dict[\"MTeams\"].columns.difference(test.columns).tolist()\n",
    "test = test.merge(data_dict[\"MTeams\"][cols_to_use].drop_duplicates(subset=[\"TeamID\"]),\n",
    "                  how=\"left\", left_on=[\"WTeamID\"], right_on=[\"TeamID\"])\n",
    "test.drop(['TeamID'], axis=1, inplace=True)\n",
    "test = test.merge(data_dict[\"MTeams\"][cols_to_use].drop_duplicates(subset=[\"TeamID\"]), \n",
    "                  how=\"left\", left_on=[\"LTeamID\"], right_on=[\"TeamID\"], suffixes=('_W', '_L'))\n",
    "test.drop(['TeamID'], axis=1, inplace=True)\n",
    "test.head()\n",
    "\n",
    "# merge with MTeamCoaches\n",
    "cols_to_use = data_dict[\"MTeamCoaches\"].columns.difference(test.columns).tolist() + [\"Season\"]\n",
    "test = test.merge(data_dict[\"MTeamCoaches\"][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]),\n",
    "                  how=\"left\", left_on=[\"Season\",\"WTeamID\"], right_on=[\"Season\",\"TeamID\"])\n",
    "test.drop(['TeamID'], axis=1, inplace=True)\n",
    "test = test.merge(data_dict[\"MTeamCoaches\"][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]), \n",
    "                  how=\"left\", left_on=[\"Season\",\"LTeamID\"], right_on=[\"Season\",\"TeamID\"], suffixes=('_W', '_L'))\n",
    "test.drop(['TeamID'], axis=1, inplace=True)\n",
    "\n",
    "# # merge with MMasseyOrdinals\n",
    "# cols_to_use = data_dict[\"MMasseyOrdinals\"].columns.difference(test.columns).tolist() + [\"Season\"]\n",
    "# test = test.merge(data_dict[\"MMasseyOrdinals\"], how=\"left\", left_on=[\"Season\",\"WTeamID\"], right_on=[\"Season\",\"TeamID\"])\n",
    "# test.drop(['TeamID'], axis=1, inplace=True)\n",
    "# test = test.merge(data_dict[\"MMasseyOrdinals\"], how=\"left\", left_on=[\"Season\",\"LTeamID\"], right_on=[\"Season\",\"TeamID\"], suffixes=('_W', '_L'))\n",
    "# test.drop(['TeamID'], axis=1, inplace=True)\n",
    "\n",
    "# merge with MNCAATourneySeeds\n",
    "cols_to_use = data_dict['MNCAATourneySeeds'].columns.difference(test.columns).tolist() + ['Season']\n",
    "test = test.merge(data_dict['MNCAATourneySeeds'][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]),\n",
    "                  how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'TeamID'])\n",
    "test.drop(['TeamID'], axis=1, inplace=True)\n",
    "test = test.merge(data_dict['MNCAATourneySeeds'][cols_to_use].drop_duplicates(subset=[\"Season\",\"TeamID\"]),\n",
    "                  how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'TeamID'], suffixes=('_W', '_L'))\n",
    "test.drop(['TeamID'], axis=1, inplace=True)\n",
    "\n",
    "print(test.shape)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_exist_in_test = [c for c in train.columns.values.tolist() if c not in test.columns.values.tolist()]\n",
    "print(not_exist_in_test)\n",
    "train = train.drop(not_exist_in_test, axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort by time (just to make sure the ordering...)\n",
    "To ensure no leak, we convert \"DayZero\" column to datetime object such that we explicitly have an order in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to datetime format\n",
    "train[\"DayZero\"] = pd.to_datetime(train[\"DayZero\"], infer_datetime_format=True)\n",
    "\n",
    "# sort by date\n",
    "train = train.sort_values(by=[\"DayZero\", \"Season\", \"DayNum\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAUTION: dealing with 'WRegularSeasonCompactResults'\n",
    "This part is very likely to be the cause of the leak. You cannot merge aggregated data containing future results with your past data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compact <- detailed (regular season files)\n",
    "regularSeason = data_dict['MRegularSeasonCompactResults']\n",
    "# regularSeason = pd.merge(data_dict['MRegularSeasonCompactResults'], data_dict['MRegularSeasonDetailedResults'], how='left',\n",
    "#              on=['Season', 'DayNum', 'WTeamID', 'WScore', 'LTeamID', 'LScore', 'WLoc', 'NumOT'])\n",
    "print(regularSeason.shape)\n",
    "regularSeason.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge with regularSeason using only the previous data\n",
    "def merge_regularSeason(df, regularSeason):\n",
    "    df_new = pd.DataFrame()\n",
    "    for i, season in enumerate(df[\"Season\"].unique()):\n",
    "        print(season)\n",
    "        if season <= 1998:\n",
    "            continue\n",
    "            \n",
    "        # split winners and losers (make sure not to use the future data!)\n",
    "        team_win_score = regularSeason.loc[regularSeason[\"Season\"] <= season, :].groupby(['WTeamID']).agg({'WScore':['sum', 'count', 'var']}).reset_index()\n",
    "        team_win_score.columns = [' '.join(col).strip() for col in team_win_score.columns.values]\n",
    "        team_loss_score = regularSeason.loc[regularSeason[\"Season\"] <= season, :].groupby(['LTeamID']).agg({'LScore':['sum', 'count', 'var']}).reset_index()\n",
    "        team_loss_score.columns = [' '.join(col).strip() for col in team_loss_score.columns.values]\n",
    "        \n",
    "        # merge with train \n",
    "        team_win_score[\"Season\"] = season\n",
    "        team_loss_score[\"Season\"] = season\n",
    "        df_fold = df.loc[df[\"Season\"] == season, :].reset_index(drop=True)\n",
    "        df_fold = df_fold.merge(team_win_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'WTeamID'])\n",
    "        df_fold = df_fold.merge(team_loss_score, how='left', left_on=['Season', 'LTeamID'], right_on=['Season', 'LTeamID'])\n",
    "        df_fold = df_fold.merge(team_loss_score, how='left', left_on=['Season', 'WTeamID'], right_on=['Season', 'LTeamID'])\n",
    "        df_fold = df_fold.merge(team_win_score, how='left', left_on=['Season', 'LTeamID_x'], right_on=['Season', 'WTeamID'])\n",
    "        df_fold.drop(['LTeamID_y', 'WTeamID_y'], axis=1, inplace=True)\n",
    "        \n",
    "        # restore\n",
    "        df_new = pd.concat([df_new, df_fold], ignore_index=True)\n",
    "    return df_new\n",
    "\n",
    "train = merge_regularSeason(train, regularSeason)\n",
    "test = merge_regularSeason(test, regularSeason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "def preprocess(df):\n",
    "    df['x_score'] = df['WScore sum_x'] + df['LScore sum_y']\n",
    "    df['y_score'] = df['WScore sum_y'] + df['LScore sum_x']\n",
    "    df['x_count'] = df['WScore count_x'] + df['LScore count_y']\n",
    "    df['y_count'] = df['WScore count_y'] + df['WScore count_x']\n",
    "    df['x_var'] = df['WScore var_x'] + df['LScore var_x']\n",
    "    df['y_var'] = df['WScore var_y'] + df['LScore var_y']\n",
    "    return df\n",
    "train = preprocess(train)\n",
    "test = preprocess(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make winner and loser train\n",
    "train_win = train.copy()\n",
    "train_los = train.copy()\n",
    "train_win = train_win[['DayNum', 'DayZero', 'Season', 'Seed_W', 'Seed_L', 'x_score', 'y_score', 'x_count', 'y_count', 'x_var', 'y_var']]\n",
    "train_los = train_los[['DayNum', 'DayZero', 'Season', 'Seed_L', 'Seed_W', 'y_score', 'x_score', 'x_count', 'y_count', 'x_var', 'y_var']]\n",
    "train_win.columns = ['DayNum', 'DayZero', 'Season', 'Seed_1', 'Seed_2', 'Score_1', 'Score_2', 'Count_1', 'Count_2', 'Var_1', 'Var_2']\n",
    "train_los.columns = ['DayNum', 'DayZero', 'Season', 'Seed_1', 'Seed_2', 'Score_1', 'Score_2', 'Count_1', 'Count_2', 'Var_1', 'Var_2']\n",
    "\n",
    "# same processing for test\n",
    "test = test[['ID', 'Season', 'Seed_W', 'Seed_L', 'x_score', 'y_score', 'x_count', 'y_count', 'x_var', 'y_var']]\n",
    "test.columns = ['ID', 'Season', 'Seed_1', 'Seed_2', 'Score_1', 'Score_2', 'Count_1', 'Count_2', 'Var_1', 'Var_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature enginnering\n",
    "def feature_engineering(df):\n",
    "    df['Seed_diff'] = df['Seed_1'] - df['Seed_2']\n",
    "    df['Seed_ratio'] = df['Seed_1'] / df['Seed_2']\n",
    "    df['Score_ratio'] = df['Score_1'] / df['Score_2']\n",
    "    df['Count_ratio'] = df['Count_1'] / df['Count_2']\n",
    "    df['Var_ratio'] = df['Var_1'] / df['Var_2']\n",
    "    df['Mean_score1'] = df['Score_1'] / df['Count_1']\n",
    "    df['Mean_score2'] = df['Score_2'] / df['Count_2']\n",
    "    df['Mean_score_diff'] = df['Mean_score1'] - df['Mean_score2']\n",
    "    df['Mean_score_ratio'] = df['Mean_score1'] / df['Mean_score2']\n",
    "    df = df.drop(['Score_1', 'Score_2', 'Count_1', 'Count_2', 'Var_1', 'Var_2'], axis=1)\n",
    "    return df\n",
    "train_win = feature_engineering(train_win)\n",
    "train_los = feature_engineering(train_los)\n",
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_win[\"result\"] = 1\n",
    "print(train_win.shape)\n",
    "train_win.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_los[\"result\"] = 0\n",
    "print(train_los.shape)\n",
    "train_los.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_win, train_los], ignore_index=True)\n",
    "data = data.sort_values(by=['DayZero', 'Season', 'DayNum']).reset_index(drop=True)\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict & Make Submission File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "We use TimeSeriesSplit to ensure no leakage due to flipping time, like training using ~2018 to predict 2019 and so on. As a model, I use a simple logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'result'\n",
    "features = data.columns.values.tolist()\n",
    "dropcols = [target,'DayNum', 'Season', 'DayZero']\n",
    "features = [f for f in features if f not in dropcols]\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict 2015 by using ~2014, for example\n",
    "models = {}\n",
    "test[\"Pred\"] = 0.5\n",
    "pred = np.zeros(test.shape[0])\n",
    "for season in test[\"Season\"].unique():\n",
    "    print(f\"Predicting {season}...\")\n",
    "    lin = RunModel(data.loc[data[\"Season\"] < season, :], test.loc[test[\"Season\"] == season, :], target, features, categoricals=[], n_splits=5, \n",
    "                   model='linear', cv_method=\"TimeSeriesSplit\", group=None, task=\"binary\", scaler=\"Standard\", seed=1220, verbose=True)\n",
    "    models[season] = lin\n",
    "    test.loc[test[\"Season\"] == season, \"Pred\"] = lin.y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('../input/google-cloud-ncaa-march-madness-2020-division-1-mens-tournament/MSampleSubmissionStage1_2020.csv')\n",
    "submission_df['Pred'] = test['Pred']\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df['Pred'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
